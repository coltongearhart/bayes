[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bayesian Statistics",
    "section": "",
    "text": "Overview\nThese course notes introduce the Bayesian approach to statistical inference for data analysis in a variety of applications. Topics include: comparison of Bayesian and frequentist methods, Bayesian model specification, prior elicitation, Markov chain Monte Carlo, Bayes factor, Model selection, hierarchical models, Bayesian regression models. Implementation of Bayesian data analysis will be done using R and Stan.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "hw-inference.html",
    "href": "hw-inference.html",
    "title": "3  HW - Inference",
    "section": "",
    "text": "Setup\nExperiment: we toss a coin 100 times and count the number of “heads” among the 100 tosses.\nNow, suppose the coin we use is very biased, and the chance of having a “head” in such a toss is 0.05. Please simulate the experiment of 100 tosses with this coin for 1000 times and analyze the outcome using Bayesian.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>HW - Inference</span>"
    ]
  },
  {
    "objectID": "hw-inference.html#section",
    "href": "hw-inference.html#section",
    "title": "\n2  HW - Inference\n",
    "section": "\n2.5 5",
    "text": "2.5 5\nRepeat part d using a Beta(1, 10) prior of the parameter \\(p\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>HW - Inference</span>"
    ]
  },
  {
    "objectID": "hw-inference.html#section-1",
    "href": "hw-inference.html#section-1",
    "title": "\n2  HW - Inference\n",
    "section": "\n2.6 6",
    "text": "2.6 6\nCompare your results in parts b, c and e. How different are they? How similar are they? Why would you see these differences?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>HW - Inference</span>"
    ]
  },
  {
    "objectID": "hw-inference.html#section-2",
    "href": "hw-inference.html#section-2",
    "title": "\n2  HW - Inference\n",
    "section": "\n2.4 3",
    "text": "2.4 3\nBased on the resulting posterior distribution, find the 95% Bayesian interval estimates (use equal tail) of the parameter using a uniform (0, 1) prior of the parameter for each of the 200 simulated experiment; Among the 200 simulations, what is the proportion of CIs including the true proportion p = 0.05? Calculate the average width of the 200 CIs.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>HW - Inference</span>"
    ]
  },
  {
    "objectID": "hw-inference.html#section-3",
    "href": "hw-inference.html#section-3",
    "title": "\n2  HW - Inference\n",
    "section": "\n2.5 4",
    "text": "2.5 4\nCompare your results in steps 2 and 3, how similar are they? How different are they? 5.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>HW - Inference</span>"
    ]
  },
  {
    "objectID": "hw-inference.html#section-4",
    "href": "hw-inference.html#section-4",
    "title": "\n2  HW - Inference\n",
    "section": "\n2.6 5",
    "text": "2.6 5\nRepeat Step 4 using a Beta(1, 10) prior of the parameter \\(p\\)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>HW - Inference</span>"
    ]
  },
  {
    "objectID": "hw-inference.html#section-5",
    "href": "hw-inference.html#section-5",
    "title": "\n2  HW - Inference\n",
    "section": "\n2.7 6",
    "text": "2.7 6\nCompare your results in steps 2, 3 and 5. How different are they? How similar are they? Why would you see these differences?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>HW - Inference</span>"
    ]
  },
  {
    "objectID": "hw-inference.html#part-a-experiment",
    "href": "hw-inference.html#part-a-experiment",
    "title": "\n2  HW - Inference\n",
    "section": "\n2.1 Part a – Experiment",
    "text": "2.1 Part a – Experiment\nDraw a boxplot of the 200 different experiment outcomes; does this plot match what you expected? Why or why not?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>HW - Inference</span>"
    ]
  },
  {
    "objectID": "hw-inference.html#part-b-frequentist-confidence-interval",
    "href": "hw-inference.html#part-b-frequentist-confidence-interval",
    "title": "\n2  HW - Inference\n",
    "section": "\n2.2 Part b – Frequentist confidence interval",
    "text": "2.2 Part b – Frequentist confidence interval\nFind the 95% interval estimates of the parameter using frequentist approach with each of the 200 simulated experiment (approximate z-CI of the population proportion):\n\\(\\hat{p} \\pm Z_{\\alpha/2} \\sqrt{\\frac{(\\hat{p}(1 - \\hat{p})}{n}}\\)\nAmong the 200 simulations, what is the proportion of CIs including the true proportion p = 0.05? Calculate the average width of the 200 CIs.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>HW - Inference</span>"
    ]
  },
  {
    "objectID": "hw-inference.html#part-c-bayesian-interval",
    "href": "hw-inference.html#part-c-bayesian-interval",
    "title": "\n2  HW - Inference\n",
    "section": "\n2.3 Part c – Bayesian interval",
    "text": "2.3 Part c – Bayesian interval\nBased on the resulting posterior distribution, find the 95% Bayesian interval estimates (use equal tail) of the parameter using a uniform (0, 1) prior of the parameter for each of the 200 simulated experiment; Among the 200 simulations, what is the proportion of CIs including the true proportion p = 0.05? Calculate the average width of the 200 CIs.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>HW - Inference</span>"
    ]
  },
  {
    "objectID": "hw-inference.html#part-d",
    "href": "hw-inference.html#part-d",
    "title": "\n2  HW - Inference\n",
    "section": "\n2.4 Part d –",
    "text": "2.4 Part d –\nCompare your results in parts b and c, how similar are they? How different are they?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>HW - Inference</span>"
    ]
  },
  {
    "objectID": "hw-inference.html#part-d-comparison",
    "href": "hw-inference.html#part-d-comparison",
    "title": "\n2  HW - Inference\n",
    "section": "\n2.4 Part d – Comparison",
    "text": "2.4 Part d – Comparison\nCompare your results in parts b and c, how similar are they? How different are they?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>HW - Inference</span>"
    ]
  },
  {
    "objectID": "hw-inference.html#part-e-new-prior",
    "href": "hw-inference.html#part-e-new-prior",
    "title": "\n2  HW - Inference\n",
    "section": "\n2.5 Part e – New prior",
    "text": "2.5 Part e – New prior\nRepeat part d using a Beta(1, 10) prior of the parameter \\(p\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>HW - Inference</span>"
    ]
  },
  {
    "objectID": "hw-inference.html#part-f-final-comparison",
    "href": "hw-inference.html#part-f-final-comparison",
    "title": "\n2  HW - Inference\n",
    "section": "\n2.6 Part f – Final comparison",
    "text": "2.6 Part f – Final comparison\nCompare your results in parts b, c and e. How different are they? How similar are they? Why would you see these differences?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>HW - Inference</span>"
    ]
  },
  {
    "objectID": "hw-inference.html#part-g-additional-insightful-plots",
    "href": "hw-inference.html#part-g-additional-insightful-plots",
    "title": "3  HW - Inference",
    "section": "Part g – Additional insightful plots",
    "text": "Part g – Additional insightful plots\nComparison of priors.\n\n# compare different prior distributions for the Bayesian credible intervals\nggplot() + \n  geom_line(aes(x = seq(from = 0, to = 1, by = 0.001),\n                y = dunif(seq(from = 0, to = 1, by = 0.001), min = 0, max = 1)),\n            color = \"green\") +\n  geom_line(aes(x = seq(from = 0, to = 1, by = 0.001),\n                y = dbeta(seq(from = 0, to = 1, by = 0.001), shape1 = 1, shape2 = 10)),\n            color = \"blue\") + \n  annotate(\"text\",\n           x = Inf, y = Inf,\n           hjust = 1, vjust = 1,\n           label = \"Uniform(0,1)\",\n           color = \"green\") + \n  annotate(\"text\",\n           x = Inf, y = Inf,\n           hjust = 1, vjust = 3,\n           label = \"Beta(1,10)\",\n           color = \"blue\") + \n  labs(title = \"Prior distributions\",\n       x = expression(theta),\n       y = expression(pi(theta)))\n\n\n\n\n\n\n\nComparison of point estimates (\\(\\hat{p}\\) for frequentist and \\(E(\\theta \\mid y)\\) for Bayes).\n\n# comparative boxplots of point estiamtes\ndata_results %&gt;% \n  select(starts_with(\"ci\")) %&gt;% \n  map(\\(col) reduce(col, bind_rows)) %&gt;% \n  map2(c(\"freq\", \"bayes_uniform\", \"bayes_beta\"),\n       \\(df, nm) select(df, pe = 1) %&gt;% # select and rename point estimate\n         mutate(method = nm,\n                .before = 1)) %&gt;% \n  reduce(bind_rows) %&gt;% \n  ggplot(aes(x = pe, y = method)) + \n  geom_boxplot() + \n  geom_vline(xintercept = p,\n             col = \"blue\") + \n  annotate(\"text\",\n           x = Inf, y = -Inf,\n           hjust = 1, vjust = -1, \n           label = \"true parameter\",\n           color = \"blue\") + \n  labs(x = \"Point estimate\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>HW - Inference</span>"
    ]
  },
  {
    "objectID": "hw-single-parameter-inference.html",
    "href": "hw-single-parameter-inference.html",
    "title": "\n5  HW - Single Parameter Inference\n",
    "section": "",
    "text": "Problem 1\nThirty randomly selected Chase customers in Cincinnati were interviewed about their experiences with Chase financial investment advising service back in 2016 and they reported the average waiting time (unit: days) until the first meeting with a financial advisor in their local branch (given below). Julia, the Chase marketing manager believes that these waiting times can be reasonably modeled with an exponential distribution, i.e., \\(Y_1, \\ldots, Y_{30} \\overset{iid}\\sim \\text{Exp}(\\lambda)\\), or \\(f(y_i \\mid \\lambda) = \\lambda\\mathrm{e}^{-\\lambda y_i}\\), where \\(i = 1, \\ldots, 30\\).\ny &lt;- c(4.3,1.0,0.5,5.2,2.5,1.6,1.3,2.5,1.2,2.3,0.8,14.4,1.7,0.5,2.2,\n        4.5,2.7,1.2,5.5,2.6,0.2,0.6,2.1,0.6,1.9,3.8,1.1,5.2,2.1,2.0)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>HW - Single Parameter Inference</span>"
    ]
  },
  {
    "objectID": "hw-single-parameter-inference.html#problem-1",
    "href": "hw-single-parameter-inference.html#problem-1",
    "title": "\n5  HW - Single Parameter Inference\n",
    "section": "",
    "text": "Part a – Prior distribution\nJulia considered a Bayesian analysis. From the historical data, on average the bank customers waited 2.5 days until their first meeting with the bank financial advisor. She assumed that the prior variance is 0.2. Please use this information to obtain an informative prior distribution for \\(\\lambda\\).\nPart b – Posterior distribution\nUse the prior in (a), find the posterior distribution for \\(\\lambda\\).\n\n# needed calculation for posterior gamma(alpha, beta)\n# -&gt; beta-post = beta-prior + sum(y)\n12.5 + sum(y)\n\n[1] 90.6",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>HW - Single Parameter Inference</span>"
    ]
  },
  {
    "objectID": "hw-single-parameter-inference.html#problem-2",
    "href": "hw-single-parameter-inference.html#problem-2",
    "title": "\n5  HW - Single Parameter Inference\n",
    "section": "Problem 2",
    "text": "Problem 2\nProgesterone level is monitored for 30 randomly chosen pregnant sheep with singletons that are 80 days pregnant.\n\ny &lt;- c(3.8,5.0,4.5,4.2,5.5,5.8,4.6,5.3,7.2,5.7,6.0,6.3,4.8,5.6,4.9,\n       4.3,4.9,4.2,3.4,4.8,5.2,5.9,5.7,2.8,6.6,6.1,9.3,7.7,5.3,7.8)\n\nJeff would like to model these measurements as normal responses from the same normal population, \\(\\text{N}(\\mu,1.52)\\). He also found out in literature that, for 80-days pregnant sheep with singletons, the average progesterone level should be centered around 8, and the range of such measurements is between 4 and 12.\nPart a – Prior distribution\nPlease suggest a prior distribution of μ for Jeff based on the literature information.\nPart b – Posterior distribution\nSpecify the posterior distribution parameters of \\(\\mu\\).\n\n# specify parameters, hyperparameters and other needed constants\nsigma &lt;- 1.5\nmu_0 &lt;- 8\ntau_0 &lt;- 4/3\nn &lt;- length(y)\n\n# calculate posterior parameters\n(tau2_n &lt;- (n / sigma^2 + 1 / tau_0^2)^(-1))\n\n[1] 0.07196402\n\n(mu_n &lt;- tau2_n * (sum(y) / sigma^2 + mu_0 / tau_0^2))\n\n[1] 5.543628\n\n\nPart c – Monte Carlo probability\nUse the Monte Carlo approach to calculate \\(P(\\mu &gt; 5 \\mid \\mathbf{y})\\).\n\n# generate sample from posterior distribution\n# -&gt; mu | y-vec ~ Normal(mu_n, tau^2_n)\ndata_post_sample &lt;- rnorm(n = 10000, mean = mu_n, sd = sqrt(tau2_n))\nhead(data_post_sample, n = 5)\n\n[1] 5.158505 6.169210 5.562049 5.398902 5.328628\n\n# calculate Monte Carlo probability\n# -&gt; P(mu &gt; 5 | y-vec)\n(prob &lt;- mean(data_post_sample &gt; 5))\n\n[1] 0.9803\n\n\nUsing Monte Carlo methods, our estimate of \\(P(\\mu &gt; 5 \\mid \\mathbf{y}) \\approx\\) 0.98\nPart d – Posterior predictive distribution\nA new sheep will be selected at random from the same population of interest. Simulate the posterior predictive distribution of the progesterone level of this new sheep. Report the estimated mean and standard deviation of the progesterone level for this new sheep based on the posterior predictive distribution you obtain.\nSimulation approach\n\n# simulate posterior predictive distribution\n\n# simulate mu's from posterior mu | y-vec\nM &lt;- 10000000\nmu_m &lt;- rnorm(n = M, mean = mu_n, sd = sqrt(tau2_n))\nhead(mu_m)\n\n[1] 5.736814 5.655383 5.491810 5.239873 5.556542 5.776568\n\n# simulate new observations from data distribution using newly sampled mu's\ny_new &lt;- rnorm(n = M, mean = mu_m, sd = sigma)\nhead(y_new)\n\n[1] 6.370977 5.323576 5.589091 6.540104 5.459827 2.666999\n\n# approximate mean and standard deviation of posterior predictive distribution\nmean(y_new) # MATCHES THEORY\n\n[1] 5.542656\n\nsd(y_new) # DOESN'T MATCH THEORY :(\n\n[1] 1.523772\n\n\nVerify with known theoretical results",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>HW - Single Parameter Inference</span>"
    ]
  },
  {
    "objectID": "hw-single-parameter-inference.html#problem-3",
    "href": "hw-single-parameter-inference.html#problem-3",
    "title": "\n5  HW - Single Parameter Inference\n",
    "section": "Problem 3",
    "text": "Problem 3\nJill is the data analyst at the Tristate Phoenix clinic, she randomly selected records of the BMI from female patients in the clinic (denoted by \\(X_1, \\ldots, X_{32}\\)) and male patients (denoted by \\(Y_1, \\ldots, Y_{28}\\)), and she assumed these two sets of records were independent normally distributed with means \\(\\mu_X, \\mu_Y\\) , and variances \\(\\sigma^2_X = \\sigma^2_Y = 1\\). Could you help her calculate a 95% credible interval (equal tails) for \\(\\mu_X − \\mu_Y\\), using the following priors \\(\\mu_X \\sim \\text{N}(\\mu = 22, \\sigma^24),\\, \\mu_Y \\sim \\text{N}(\\mu = 20.5, \\sigma^2 = 4)\\) on \\(\\mu_X, \\mu_Y\\)?\n\nx = c(22.7,21.5,23.1,21.7,23.2,22.4,20.3,23.7,23.2,22.3,24.3,\n      22.3,22.9,22.1,22.3,21.2,23.5,21.5,21.6,20.9,20.0,20.0,\n      21.1,24.1,22.6,22.2,21.3,20.8,22.0,22.9,23.2,20.5)\n\ny = c(22.0,20.3,20.6,20.4,20.5,20.0,18.9,20.2,21.4,21.4,18.5,20.8,22.8,19.1,\n      18.8,19.7,21.0,20.2,21.3,20.5,19.2,20.6,20.1,18.9,20.9,19.6,21.3,21.4)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>HW - Single Parameter Inference</span>"
    ]
  },
  {
    "objectID": "hw-single-parameter-inference.html#problem-4",
    "href": "hw-single-parameter-inference.html#problem-4",
    "title": "\n5  HW - Single Parameter Inference\n",
    "section": "Problem 4",
    "text": "Problem 4\nSuppose \\(Y \\mid \\theta \\sim \\text{Poisson}(\\theta)\\). Find the Jeffrey’s prior for \\(\\theta\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>HW - Single Parameter Inference</span>"
    ]
  },
  {
    "objectID": "lab-inference.html",
    "href": "lab-inference.html",
    "title": "\n2  Lab - Inference\n",
    "section": "",
    "text": "Setup\nExperiment: we toss a coin 100 times and count the number of “heads” among the 100 tosses.\nNow, suppose the chance of having a “head” in such a toss is 0.50 for a particular coin. Let’s simulate the 100 tosses and analyze the outcome using Bayesian!\nThe goal of this first lab will be:\n- Become familiar with R and RStudio - Practice with vector in R - Begin to explore simulation study in R - Use computational tools to find the Bayesian point estimate and interval estimate\nStep 1 – Single experiment\nSimulate the outcomes of the 100 tosses.\n\nset.seed(1)\n\n# simulate 100 tosses of a coin individually\n# -&gt; heads = 1 and tails = 0\ndata_experiment &lt;- rbinom(n = 100, size = 1, p = 0.5)\n\nStep 2 – Response\nFind the number of “heads” and plug that into the posterior distribution of the parameter.\n\n# count the number of heads\ny &lt;- sum(data_experiment == 1)\n\n\n# alternatively we could directly simulate the number of heads out of 100\ny &lt;- rbinom(n = 1, size = 100, prob = 0.5)\n\nAs shown in the Chapter 1 and in Chapter 3, \\(\\theta \\mid y \\sim \\text{Beta}(y + 1, 101 - y)\\) with prior \\(\\theta \\sim \\text{Uniform}(0,1)\\).\n\n# compare the prior distribution and the posterior distribution\nggplot() + \n  geom_line(aes(x = seq(from = 0, to = 1, by = 0.001),\n                y = dunif(seq(from = 0, to = 1, by = 0.001), min = 0, max = 1)),\n            color = \"steelblue4\") +\n  geom_line(aes(x = seq(from = 0, to = 1, by = 0.001),\n                y = dbeta(seq(from = 0, to = 1, by = 0.001), shape1 = y+1, shape2 = 101-y)),\n            color = \"orange\") + \n  annotate(\"text\",\n           x = Inf, y = Inf,\n           hjust = 1, vjust = 1,\n           label = \"Prior = Uniform(0,1)\",\n           color = \"steelblue4\") + \n  annotate(\"text\",\n           x = Inf, y = Inf,\n           hjust = 1, vjust = 3,\n           label = \"Posterior = Beta(y+1,101-y)\",\n           color = \"orange\") + \n  labs(x = expression(theta),\n       y = expression(pi(theta)))\n\n\n\n\n\n\n\nStep 3 – Frequentist point estimate\nFind the point estimate of the parameter using frequentist approach (relative frequency of the “head” among 100 tosses).\n\n# calculate frequentist point estimate p-hat = x / n\n(pe_freq &lt;- y / 100)\n\n[1] 0.48\n\n\nStep 4 – Frequentist confidence interval\nFind the 95% interval estimate of the parameter using frequentist approach (approximate z-CI of the population proportion):\n\\(\\hat{p} \\pm Z_{\\alpha/2} \\sqrt{\\frac{(\\hat{p}(1 - \\hat{p})}{n}}\\)\n\n# calculate 95% confidence interval\npe_freq + c(-1, 1) * qnorm(0.975) * sqrt(pe_freq * (1 - pe_freq) / 100)\n\n[1] 0.3820802 0.5779198\n\n\nMore choices of modified frequentist PE’s available from package binom in R.\n\n# calculate other frequentist confidence intervals\nbinom::binom.confint(x = y, n = 100, conf.level = 0.95)\n\n\n  \n\n\n\nNote that our manual calculation matches the ‘asymptotic’ interval from above.\nStep 5 – Bayes’ point estimate\nBased on the resulting posterior distribution, find the point estimate of the parameter.\nNote that \\(E(\\theta \\mid y) = \\frac{\\alpha}{\\alpha + \\beta} = \\frac{y + 1}{(y + 1) + (101 - y)} = \\frac{y + 1}{102}\\).\n\n# calculate bayes point estimate\n# -&gt; posterior mean\n(pe_bayes_mean &lt;- (y + 1) / 102)\n\n[1] 0.4803922\n\n# -&gt; posterior median\n(pe_bayes_median &lt;- qbeta(0.5, shape1 = y + 1, shape2 = 101 - y))\n\n[1] 0.4802635\n\n# -&gt; posterior mode\n# --&gt; one method\ntheta &lt;- seq(from = 0, to = 1, by = 0.001)\nd_posterior &lt;- dbeta(theta, shape1 = y + 1, shape2 = 101 - y)\nplot(x = theta, y = d_posterior, type = \"l\", xlab = expression(theta), ylab = expression(f(theta|y))) # unimodal\n\n\n\n\n\n\n(pe_bayes_mode &lt;- theta[which.max(d_posterior)])\n\n[1] 0.48\n\n# --&gt; another method\nf_x_beta &lt;- function(x, y) {\n  dbeta(x, shape1 = y + 1, shape2 = 101 - y)\n}\noptimize(f = f_x_beta, interval = c(0,1), y = y, maximum = TRUE)\n\n$maximum\n[1] 0.4799847\n\n$objective\n[1] 8.044908\n\n\nStep 6 – Bayes’ Interval\nBased on the resulting posterior distribution, find the 95% interval estimate of the parameter.\n\n# calculate bayes interval\n# -&gt; equal tails\nqbeta(p = c(0.025,0.975), shape1 = y+1, shape2 = 101-y)\n\n[1] 0.3844742 0.5770388\n\n# -&gt; HPD interval\nTeachingDemos::hpd(qbeta, shape1 = y + 1, shape2 = 101- y, conf = 0.95)\n\n[1] 0.3842179 0.5767799\n\n\nNote these don’t match the ‘bayes’ interval from binom.confint() because it is using a prior \\(\\theta \\sim \\text{Beta}(\\alpha =0.5, \\beta = 0.5)\\).\nAlso note that the HPD interval function is not a general way of finding the HPD credible intervals because it is limited to the case when we know the inverse CDF function (i.e. q&lt;dist&gt;()) of the posterior distributions and it is not always tractable.\nThe following code gives a general way of doing it, in which the “ruler” data object is basically the horizontal line segment that will be “dropped” onto the posterior density and we try to find such a “ruler” that will exactly gives HPD area of \\(1 − \\alpha\\).\nStep 7 – General HPD interval\nBelow we implement a general search procedure for finding the parameter values that correspond to the HPD interval. We just need to have the cdf of the posterior density rather than the inverse cdf like in TeachingDemos::hpd().\n\n# HPD calculation (more general approach)\n\n# specify parameter values and corresponding posterior density values (rounded to 2 decimal places so can check equality later)\ntheta &lt;- seq(from = 0, to = 1, length = 5000)\nd_posterior &lt;- dbeta(theta, shape1 = y+1, shape2= 101-y) %&gt;% round(2)\n\n# specify target -&gt; the desired coverage probability level for the interval\n# specify tolerance -&gt; variation in the desired coverage probability for the numerical search of\n# -&gt; number chosen according to the precision (decimal places) of the posterior density function values found above\ntarget &lt;- 0.95\ntol &lt;- 0.005\n\n# loop to find HPD interval\n# initialize values\n# -&gt; indicator of when search is done\n# -&gt; start counter at an index for lower bound -&gt; starting in the middle and working outwards (to find the narrowest instance in which HPD criteria is met)\ninterval_found &lt;- FALSE\ni &lt;- length(theta)/2\nwhile (i &gt; 0 & !interval_found) { # continue searching lower bounds across all parameter values or until desired interval is found\n  \n  # initialize starting index for the upper bound\n  j &lt;- length(theta)/2\n  \n  # search across parameter values for the upper bound\n  while (j &lt;= length(theta) & !interval_found){\n    \n    # check if the upper bound has an equivalent density height to the lower bound\n    if (d_posterior[i] == d_posterior[j]){\n      \n      # calculate needed values for coverage probability using cdf\n      F_lower &lt;- pbeta(theta[i], shape1 = y+1, shape2= 101-y)\n      F_upper &lt;- pbeta(theta[j], shape1 = y+1, shape2= 101-y)\n      \n      # evaluate coverage probability and check if: F(upper) - F(lower) = target +- tolerance\n      if (between(F_upper - F_lower, target - tol, target + tol)) {\n        interval_found &lt;- TRUE  \n      }\n      \n    }\n    \n    # increase upper bound\n    j &lt;- j+1\n  }\n  \n  # decrease lower bound (working inside out)\n  # -&gt; and start over with the upper bounds\n  i &lt;- i-1\n  \n}\n\n# posterior density value corresponding to the HPD region boundaries\n(k &lt;- d_posterior[i])\n\n[1] 1.3\n\n# HPD\nbound_lower_hpd &lt;- theta[i]\nbound_upper_hpd &lt;- theta[j]\npaste0(\"Index of \", target*100, \"% HPD interval boundaries: \", \"i = \", i, \" and j = \", j)\n\n[1] \"Index of 95% HPD interval boundaries: i = 1930 and j = 2875\"\n\npaste0(target*100, \"% HPD interval: [\", round(bound_lower_hpd, 3), \", \", round(bound_upper_hpd, 3), \"]\")\n\n[1] \"95% HPD interval: [0.386, 0.575]\"\n\n\nStep 8 – Simulation\nRepeat the simulation of data and analysis steps 1-6 for 50 times, record all the analysis outcomes.\n\n# define function to calculate everything (95% confidence)\n# -&gt; freq: p-hat = x / n, asymptotic interval estimates\n# -&gt; bayes: posterior mean, equal tails interval estimates\ncalc_values &lt;- function(y) {\n  \n  data.frame(y = y) %&gt;% \n    mutate(\n      pe_freq = y / 100,\n      bound_lower_freq = pe_freq - qnorm(0.975) * sqrt(pe_freq * (1 - pe_freq) / 100),\n      bound_upper_freq = pe_freq + qnorm(0.975) * sqrt(pe_freq * (1 - pe_freq) / 100),\n      pe_bayes = (y + 1) / 102,\n      bound_lower_bayes = qbeta(p = 0.025, shape1 = y+1, shape2 = 101-y),\n      bound_upper_bayes = qbeta(p = 0.975, shape1 = y+1, shape2 = 101-y)\n    )\n  \n}\n\n# simulate the number of heads many times\ny &lt;- rbinom(n = 50, size = 100, prob = 0.5)\n\n# run simulation\nresults &lt;- y %&gt;% \n  map(\\(y_i) calc_values(y_i)) %&gt;% \n  reduce(bind_rows) %&gt;% \n  mutate(i = row_number(),\n         .before = 1)\nresults %&gt;% round(3) %&gt;% head",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Lab - Inference</span>"
    ]
  },
  {
    "objectID": "index.html#resources",
    "href": "index.html#resources",
    "title": "Bayesian Statistics",
    "section": "Resources",
    "text": "Resources\n\nLecture notes are based on “Bayesian Data Analysis 3e” by Gelman, et al.\nGreat book also based on this book with r code examples: Bayesian inference\n\n\n\n\n\n\n\nQuarto blog publish details\n\n\n\nThis book was created using Quarto and published with Github Pages.\n\n\n\n\n\n\n\n\nGithub repository for code\n\n\n\nYou can find the code to reproduce this project at coltongearhart/bayes.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "lab-prediction.html",
    "href": "lab-prediction.html",
    "title": "\n5  Lab - Single Parameter Inference\n",
    "section": "",
    "text": "Setup\nLet’s revisit the coin toss example today. We will work on it with multiple approaches. This time, let’s just assume we tossed the coin 100 times and get 50 (\\(Y = 50\\)) heads.\n\nData distribution: \\(Y \\mid p \\sim Bin(100, p)\\)\n\nPrior: \\(p \\sim \\text{Uniform}(0,1)\\)\n\nObserved data: \\(y = 50\\)\n\nStep 1 – Estimation of parameters using known posterior distribution\nIf we know the math and used integration to find the posterior:\n\nPosterior: \\(p \\mid y = 50 \\sim \\text{Beta}(51, 51)\\)\n\nSimulate the outcomes of the 100 tosses.\n\n\n# shape of the posterior distribution\ncurve(dbeta(x, shape1 = 51, shape2 = 51), from = 0, to = 1)\n\n\n\n\n\n\n\n\n# point estimates\n(posterior_mean &lt;- 51 / 102)\n\n[1] 0.5\n\n(posterior_median &lt;- qbeta(p = 0.50, shape1 = 51, shape2 = 51))\n\n[1] 0.5\n\nx &lt;- seq(from = 0, to = 1, length = 300)\nd_posterior &lt;- dbeta(x, shape1 = 51, shape2 = 51)\n(posterior_mode &lt;- x[which.max(d_posterior)])\n\n[1] 0.4983278\n\n\n\n# credible intervals\n\n# quantile based 95% confidence interval \nqbeta(p = c(0.025,0.975), shape1 = 51, shape2 = 51)\n\n[1] 0.4036431 0.5963569\n\n# 95% HPD CI\nTeachingDemos::hpd(qbeta, shape1 = 51, shape2 = 51, conf = 0.95)\n\n[1] 0.4036431 0.5963569\n\n\nStep 2 – Estimation of parameters using simulation\nSimulation is commonly used when the posterior doesn’t have an explicit form. This process is illustrated below (although this one does have a closed form solution that we can use to validate the results).\n\nNote that we can always write down the posterior distribution because it is just the product of the data distribution and the prior (which we always know / assume), but often the explicit for of the posterior is not a known form (e.g. \\(\\sim \\text{Beta}\\) or \\(\\sim \\text{Normal}\\).\n\nIf we do not know the math, we can still find the posterior, but we won’t have the exact posterior distribution, instead we are able to generate posterior samples. This is easy to implement and use when we have a simple model and single parameter.\n\n\nFirstly, we create a sequence of possible values of the parameter (discretize the parameter space of \\(p\\)).\n\nNote that the method of discretizing the parameter space is not a very good method in practice and it cannot be used when the parameter space is unbounded.\n\n\n\nSecondly, we construct the prior density values for each of the possible values and using Bayes Theorem, we combine the prior density and likelihood function into the posterior.\n\nNote that we would numerically sum the products of prior density and likelihood values to find the normalizing constant \\(p(y)\\).\n\n\nThirdly, simulate random draws from the approximate posterior distribution.\n\n\n\n# 1) create a sequence of possible values for the parameter\np &lt;- seq(from = 0.001, to = 0.999, by = 0.001)\n\n# 2) construct the posterior, assuming y = 50 is observed\n# =&gt; posterior = data dist * prior dist\n# -&gt; getting the functional value at each p\nf_p_post &lt;- dbinom(x = 50, size = 100, prob = p) * dunif(p)\n\n# 3) simulate p from the posterior distribution\n# -&gt; randomly sample according to weights\nn_sim &lt;- 100000\np_post &lt;- sample(x = p, size = n_sim, replace = TRUE, prob = f_p_post)\n\n# view approximate distribution\nhist(p_post)\n\n\n\n\n\n\n\n\n# point estimates\n(posterior_mean_sim &lt;- mean(p_post))\n\n[1] 0.4998291\n\n(posterior_median_sim &lt;- median(p_post))\n\n[1] 0.5\n\n(posterior_mode_sim &lt;- p[which.max(f_p_post)])\n\n[1] 0.5\n\n\n\n# credible interval\n# -&gt; quantile based 95% confidence interval \nquantile(p_post, c(0.025, 0.975))\n\n 2.5% 97.5% \n0.403 0.596 \n\n\nStep 3 – Simulation-based prediction of future observations\nYou certainly can predict your future observations based on the posterior predictive distribution by doing the math here. But it is often difficult to analytically solve with calculus. On the opposite, it is fairly easy to get the posterior predictive distribution using simulation.\nNote that we have simulated \\(p\\) from the posterior distribution and saved them in the vector p_post, so let’s directly use them and simulate the future observation conditional on each one of these \\(p\\)’s to get an idea of the posterior predictive distribution.\n\n\np_post can also be simulated directly from \\(\\text{Beta}(51, 51)\\) (the posterior distribution shown in @notes-inference) via p_post = rbeta(nsim, 51,51). But the process described above is an approximation of simulating from this.\n\nOne toss: We already analytically figured out the posterior predictive distribution of 1 additional coin toss in class, so now we can numerically compare ̃the simulation vs analytical answer (\\(P(\\tilde{Y} = 1 \\mid Y = 50) = 0.5\\)):\n\n# generate an experimental value of the coin toss for each of the n_sim = 100,000 values of p from the posterior distribution\n# -&gt; need to make sure that the number of predicted responses should match the number of posterior samples of the parameter!\ny_pred_post_1_sim &lt;- rbinom(n = n_sim, size = 1, prob = p_post)\n\n# summarize the posterior predictive distribution\n# -&gt; frequency table of the future observations table\ntable(y_pred_post_1_sim)\n\ny_pred_post_1_sim\n    0     1 \n49861 50139 \n\n# calculate the naive posterior predictive distribution table using relative frequency\ntable(y_pred_post_1_sim) / length(y_pred_post_1_sim)\n\ny_pred_post_1_sim\n      0       1 \n0.49861 0.50139 \n\n\n5 tosses: Now let’s assume that we plan to toss the coin for another 5 times.\n\n# same as above, except for 5 tosses\ny_pred_post_5_sim &lt;- rbinom(n = n_sim, size = 5, prob = p_post)\ntable(y_pred_post_5_sim)\n\ny_pred_post_5_sim\n    0     1     2     3     4     5 \n 3501 15912 30677 30740 15762  3408 \n\ntable(y_pred_post_5_sim) / length(y_pred_post_5_sim)\n\ny_pred_post_5_sim\n      0       1       2       3       4       5 \n0.03501 0.15912 0.30677 0.30740 0.15762 0.03408 \n\n\n\n\n# calculate analytical result\n# -&gt; y-pred_post | y-vec = 101! (50 + y-tilde)! (55 - y-tilde)! 5! / (50! 50! 106! y-tilde! (5 - y-tilde)!)\ny_pred_post_5 &lt;- function(y) {\n  factorial(101) * factorial(50+y) * factorial(55-y) *  factorial(5) / (factorial(50) * factorial(50) * factorial(106) * factorial(y) * factorial(5-y))\n}\n\n# calculate probabilities for each possible y-tilde value (number of successes out of 5 tosses)\nsapply(0:5, y_pred_post_5)\n\n[1] 0.03432732 0.15915395 0.30651872 0.30651872 0.15915395 0.03432732\n\n\n100 tosses: Let’s assume that we plan to toss the coin for another 100 times first.\n\n# same as above, except for 100 tosses\ny_pred_post_100_sim &lt;- rbinom(n = n_sim, size = 100, prob = p_post)\n\n# create pmf plot\n# -&gt; sort predicted values to match order returned from table()\nplot(x = y_pred_post_100_sim %&gt;% unique %&gt;% sort,\n     y = y_pred_post_100_sim %&gt;% table %&gt;% as.numeric %&gt;% divide_by(n_sim),\n     type = \"h\", xlab = \"Y-pred\", ylab = \"freq\")\n\n\n\n\n\n\n\n\n# calculate analytical result\n# -&gt; (not simplifying to factorials) y-pred_post | y-vec = C(100,y-tilde) beta(51 + y-tilde, 51 - y-tilde) / beta(51, 51)\n\ny_pred_post_100 &lt;- function(y) {\n\n  choose(100,y) * beta(51+y,151-y)/beta(51,51)\n\n}\n\n# calculate probabilities for each possible y-tilde value (number of successes out of 100 tosses)\nsapply(0:100, y_pred_post_100)\n\n  [1] 1.117015e-19 3.797850e-18 6.560849e-17 7.675012e-16 6.837026e-15\n  [6] 4.945136e-14 3.023922e-13 1.607359e-12 7.578753e-12 3.218894e-11\n [11] 1.246465e-10 4.443569e-10 1.470001e-09 4.542745e-09 1.318770e-08\n [16] 3.613687e-08 9.385549e-08 2.318783e-07 5.466671e-07 1.233270e-06\n [21] 2.668948e-06 5.552976e-06 1.112943e-05 2.152547e-05 4.024020e-05\n [26] 7.281560e-05 1.277074e-04 2.173466e-04 3.593414e-04 5.777088e-04\n [31] 9.039631e-04 1.377815e-03 2.047184e-03 2.967207e-03 4.197949e-03\n [36] 5.800615e-03 7.832231e-03 1.033899e-02 1.334871e-02 1.686314e-02\n [41] 2.085104e-02 2.524317e-02 2.993008e-02 3.476375e-02 3.956334e-02\n [46] 4.412515e-02 4.823618e-02 5.169008e-02 5.430386e-02 5.593363e-02\n [51] 5.648743e-02 5.593363e-02 5.430386e-02 5.169008e-02 4.823618e-02\n [56] 4.412515e-02 3.956334e-02 3.476375e-02 2.993008e-02 2.524317e-02\n [61] 2.085104e-02 1.686314e-02 1.334871e-02 1.033899e-02 7.832231e-03\n [66] 5.800615e-03 4.197949e-03 2.967207e-03 2.047184e-03 1.377815e-03\n [71] 9.039631e-04 5.777088e-04 3.593414e-04 2.173466e-04 1.277074e-04\n [76] 7.281560e-05 4.024020e-05 2.152547e-05 1.112943e-05 5.552976e-06\n [81] 2.668948e-06 1.233270e-06 5.466671e-07 2.318783e-07 9.385549e-08\n [86] 3.613687e-08 1.318770e-08 4.542745e-09 1.470001e-09 4.443569e-10\n [91] 1.246465e-10 3.218894e-11 7.578753e-12 1.607359e-12 3.023922e-13\n [96] 4.945136e-14 6.837026e-15 7.675012e-16 6.560849e-17 3.797850e-18\n[101] 1.117015e-19\n\n# compare to simulated results\n# -&gt; get the theoretical functional values for the unique simulated posterior predictive values\nplot(x = y_pred_post_100_sim %&gt;% unique %&gt;% sort %&gt;% sapply(y_pred_post_100),\n     y = y_pred_post_100_sim %&gt;% table %&gt;% as.numeric %&gt;% divide_by(n_sim), \n     xlab = \"analytical\", ylab = \"simulated\")\nabline(0,1)\n\n\n\n\n\n\n\n\n# point estimation/prediction for the future observations\nmean(y_pred_post_100_sim)\n\n[1] 50.00089\n\n# prediction interval\nquantile(y_pred_post_100_sim, c(0.025,0.975))\n\n 2.5% 97.5% \n   36    64 \n\n\nCompare the posterior predictive samples vs the prior predictive samples. First we need to simulate \\(p\\) from the prior distribution, then simulate the future observation conditional on each one of these \\(p\\)’s to get an idea of the prior predictive distribution.\n\n# generate an experimental value of the future 100 tosses for each p\np_prior &lt;- sample(x = p, size = n_sim, replace = TRUE, prob = dunif(p))\ny_pred_prior_100_sim &lt;- rbinom(n = n_sim, size = 100, prob = p_prior)\n\n# calculate point estimate and interval\nmean(y_pred_prior_100_sim)\n\n[1] 50.00801\n\nquantile(y_pred_prior_100_sim, c(.025,.975))\n\n 2.5% 97.5% \n    2    98 \n\n\n\n# compare with what we got earlier:\n\n# compare the two distribution via density curves \nplot(density(y_pred_post_100_sim), xlim = c(0,100), main = \"Prior/posterior pred. dist. of 100 future tosses\")\nlines(density(y_pred_prior_100_sim),col = \"red\")",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Lab - Single Parameter Inference</span>"
    ]
  }
]