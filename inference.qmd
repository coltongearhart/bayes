# Inference 

## Notes 
<embed src="lectures/notes-inference.pdf" type="application/pdf" width="100%" height="1000px"></embed>

## Lab

```{r}
#| label: load-prereqs
#| echo: false
#| message: false

# knitr options
source("_common.R")

```

### Setup

*Experiment: we toss a coin 100 times and count the number of “heads” among the 100 tosses.*

*Now, suppose the chance of having a "head" in such a toss is 0.50 for a particular coin. Let’s simulate the 100 tosses and analyze the outcome using Bayesian!*

*The goal of this first lab will be:*

*- Become familiar with R and RStudio*
*- Practice with vector in R*
*- Begin to explore simulation study in R*
*- Use computational tools to find the Bayesian point estimate and interval estimate*

### Step 1 -- Single experiment

*Simulate the outcomes of the 100 tosses.*

```{r}

set.seed(1)

# simulate 100 tosses of a coin individually
# -> heads = 1 and tails = 0
data_experiment <- rbinom(n = 100, size = 1, p = 0.5)

```

### Step 2 -- Response

*Find the number of “heads” and plug that into the posterior distribution of the parameter.*

```{r}

# count the number of heads
y <- sum(data_experiment == 1)

```

```{r}
#| eval: false

# alternatively we could directly simulate the number of heads out of 100
y <- rbinom(n = 1, size = 100, prob = 0.5)

```

As shown in the notes and in the homework, $\theta \mid y \sim \text{Beta}(y + 1, 101 - y)$ with prior $\theta \sim \text{Uniform}(0,1)$.

```{r}

# compare the prior distribution and the posterior distribution
ggplot() + 
  geom_line(aes(x = seq(from = 0, to = 1, by = 0.001),
                y = dunif(seq(from = 0, to = 1, by = 0.001), min = 0, max = 1)),
            color = "steelblue4") +
  geom_line(aes(x = seq(from = 0, to = 1, by = 0.001),
                y = dbeta(seq(from = 0, to = 1, by = 0.001), shape1 = y+1, shape2 = 101-y)),
            color = "orange") + 
  annotate("text",
           x = Inf, y = Inf,
           hjust = 1, vjust = 1,
           label = "Prior = Uniform(0,1)",
           color = "steelblue4") + 
  annotate("text",
           x = Inf, y = Inf,
           hjust = 1, vjust = 3,
           label = "Posterior = Beta(y+1,101-y)",
           color = "orange") + 
  labs(x = expression(theta),
       y = expression(pi(theta)))

```

### Step 3 -- Frequentist point estimate

*Find the point estimate of the parameter using frequentist approach (relative frequency of the "head" among 100 tosses).*

```{r}

# calculate frequentist point estimate p-hat = x / n
(pe_freq <- y / 100)

```

### Step 4 -- Frequentist confidence interval

*Find the 95% interval estimate of the parameter using frequentist approach (approximate z-CI of the population proportion):*

$\hat{p} \pm Z_{\alpha/2} \sqrt{\frac{(\hat{p}(1 - \hat{p})}{n}}$

```{r}

# calculate 95% confidence interval
pe_freq + c(-1, 1) * qnorm(0.975) * sqrt(pe_freq * (1 - pe_freq) / 100)

```

More choices of modified frequentist PE’s available from package `binom` in R.

```{r}

# calculate other frequentist confidence intervals
binom::binom.confint(x = y, n = 100, conf.level = 0.95)

```

Note that our manual calculation matches the 'asymptotic' interval from above.

### Step 5 -- Bayes' point estimate

*Based on the resulting posterior distribution, find the point estimate of the parameter.*

Note that $E(\theta \mid y) = \frac{\alpha}{\alpha + \beta} = \frac{y + 1}{(y + 1) + (101 - y)} = \frac{y + 1}{102}$.

```{r}

# calculate bayes point estimate
# -> posterior mean
(pe_bayes_mean <- (y + 1) / 102)

# -> posterior median
(pe_bayes_median <- qbeta(0.5, shape1 = y + 1, shape2 = 101 - y))

# -> posterior mode
# --> one method
theta <- seq(from = 0, to = 1, by = 0.001)
d_posterior <- dbeta(theta, shape1 = y + 1, shape2 = 101 - y)
plot(x = theta, y = d_posterior, type = "l", xlab = expression(theta), ylab = expression(f(theta|y))) # unimodal
(pe_bayes_mode <- theta[which.max(d_posterior)])

# --> another method
f_x_beta <- function(x, y) {
  dbeta(x, shape1 = y + 1, shape2 = 101 - y)
}
optimize(f = f_x_beta, interval = c(0,1), y = y, maximum = TRUE)

```

### Step 6 -- Bayes' Interval

*Based on the resulting posterior distribution, find the 95% interval estimate of the parameter.*

```{r}

# calculate bayes interval
# -> equal tails
qbeta(p = c(0.025,0.975), shape1 = y+1, shape2 = 101-y)

# -> HPD interval
TeachingDemos::hpd(qbeta, shape1 = y + 1, shape2 = 101- y, conf = 0.95)

```

Note these don't match the 'bayes' interval from `binom.confint()` because it is using a prior $\theta \sim \text{Beta}(\alpha =0.5, \beta = 0.5)$.

Also note that the HPD interval function is not a general way of finding the HPD credible intervals because it is limited to the case when we know the inverse CDF function (i.e. `q<dist>()`) of the posterior distributions and it is not always tractable.

The following code gives a general way of doing it, in which the "ruler" data object is basically the horizontal line segment that will be "dropped" onto the posterior density and we try to find such a "ruler" that will exactly gives HPD area of $1 − \alpha$.

### Step 7 -- General HPD interval

Below we implement a general search procedure for finding the parameter values that correspond to the HPD interval. We just need to have the cdf of the posterior density rather than the inverse cdf like in `TeachingDemos::hpd()`.

```{r}

# HPD calculation (more general approach)

# specify parameter values and corresponding posterior density values (rounded to 2 decimal places so can check equality later)
theta <- seq(from = 0, to = 1, length = 5000)
d_posterior <- dbeta(theta, shape1 = y+1, shape2= 101-y) %>% round(2)

# specify target -> the desired coverage probability level for the interval
# specify tolerance -> variation in the desired coverage probability for the numerical search of
# -> number chosen according to the precision (decimal places) of the posterior density function values found above
target <- 0.95
tol <- 0.005

# loop to find HPD interval
# initialize values
# -> indicator of when search is done
# -> start counter at an index for lower bound -> starting in the middle and working outwards (to find the narrowest instance in which HPD criteria is met)
interval_found <- FALSE
i <- length(theta)/2
while (i > 0 & !interval_found) { # continue searching lower bounds across all parameter values or until desired interval is found
  
  # initialize starting index for the upper bound
  j <- length(theta)/2
  
  # search across parameter values for the upper bound
  while (j <= length(theta) & !interval_found){
    
    # check if the upper bound has an equivalent density height to the lower bound
    if (d_posterior[i] == d_posterior[j]){
      
      # calculate needed values for coverage probability using cdf
      F_lower <- pbeta(theta[i], shape1 = y+1, shape2= 101-y)
      F_upper <- pbeta(theta[j], shape1 = y+1, shape2= 101-y)
      
      # evaluate coverage probability and check if: F(upper) - F(lower) = target +- tolerance
      if (between(F_upper - F_lower, target - tol, target + tol)) {
        interval_found <- TRUE  
      }
      
    }
    
    # increase upper bound
    j <- j+1
  }
  
  # decrease lower bound (working inside out)
  # -> and start over with the upper bounds
  i <- i-1
  
}

# posterior density value corresponding to the HPD region boundaries
(k <- d_posterior[i])

# HPD
bound_lower_hpd <- theta[i]
bound_upper_hpd <- theta[j]
paste0("Index of ", target*100, "% HPD interval boundaries: ", "i = ", i, " and j = ", j)
paste0(target*100, "% HPD interval: [", round(bound_lower_hpd, 3), ", ", round(bound_upper_hpd, 3), "]")

```

### Step 8 -- Simulation

*Repeat the simulation of data and analysis steps 1-6 for 50 times, record all the analysis outcomes.*

```{r}

# define function to calculate everything (95% confidence)
# -> freq: p-hat = x / n, asymptotic interval estimates
# -> bayes: posterior mean, equal tails interval estimates
calc_values <- function(y) {
  
  data.frame(y = y) %>% 
    mutate(
      pe_freq = y / 100,
      bound_lower_freq = pe_freq - qnorm(0.975) * sqrt(pe_freq * (1 - pe_freq) / 100),
      bound_upper_freq = pe_freq + qnorm(0.975) * sqrt(pe_freq * (1 - pe_freq) / 100),
      pe_bayes = (y + 1) / 102,
      bound_lower_bayes = qbeta(p = 0.025, shape1 = y+1, shape2 = 101-y),
      bound_upper_bayes = qbeta(p = 0.975, shape1 = y+1, shape2 = 101-y)
    )
  
}

# simulate the number of heads many times
y <- rbinom(n = 50, size = 100, prob = 0.5)

# run simulation
results <- y %>% 
  map(\(y_i) calc_values(y_i)) %>% 
  reduce(bind_rows) %>% 
  mutate(i = row_number(),
         .before = 1)
results %>% round(3) %>% head

```


## Homework

### Setup

*Experiment: we toss a coin 100 times and count the number of “heads” among the 100 tosses.*

*Now, suppose the coin we use is very biased, and the chance of having a “head” in such a toss is 0.05. Please simulate the experiment of 100 tosses with this coin for 1000 times and analyze the outcome using Bayesian.*

### Part a -- Experiment

*Draw a boxplot of the 1000 different experiment outcomes; does this plot match what you expected? Why or why not?*

```{r}

set.seed(1)

# simulate coin 100 coin tosses 1000 times
p <- 0.05
n <- 100
m <- 1000
data_results <- rbinom(n = m, size = n, prob = p) %>% 
  data.frame(y = .)

# draw boxplot of results
boxplot(data_results$y, main = "Number of heads", horizontal = TRUE)

```

The boxplot is more or less what I expected, the median is very close the the expected number of heads out of 100 tosses. Perhaps expected it to be more right skewed because the true population proportion is so small.

### Part b -- Frequentist confidence interval

*Find the 95% interval estimates of the parameter using frequentist approach with each of the 1000 simulated experiment (approximate z-CI of the population proportion):*

$\hat{p} \pm Z_{\alpha/2} \sqrt{\frac{(\hat{p}(1 - \hat{p})}{n}}$

*Among the 1000 simulations, what is the proportion of CIs including the true proportion p = 0.05? Calculate the average width of the 1000 CIs.*

```{r}

# define function to calculate margin of error
calc_moe <- function(p_hat, n, c) {
  qnorm((1 - c) / 2, lower.tail = FALSE) * sqrt(p_hat * (1 - p_hat) / n)
}

# calculate 95% frequentist confidence interval
c <- 0.95
data_results %<>% 
  mutate(p_hat = y / n,
         bound_lower = p_hat - calc_moe(p_hat, n, c),
         bound_upper = p_hat + calc_moe(p_hat, n, c),
         capture = bound_lower < p & bound_upper > p)

# display first few CIs
data_results %>% display_nice(n = 5)

# nest results
data_results %<>% 
  mutate(i = row_number(),
         .before = y) %>% 
  nest(ci_freq = c(p_hat, bound_lower, bound_upper, capture))
head(data_results, n = 5)

# calculate capture rate
data_results$ci_freq %>% 
  reduce(bind_rows) %>% 
  summarize(avg_width = mean(bound_upper - bound_lower),
            capture_rate = mean(capture))

```

### Part c -- Bayesian interval

*Based on the resulting posterior distribution, find the 95% Bayesian interval estimates (use equal tail) of the parameter using a uniform (0, 1) prior of the parameter for each of the 1000 simulated experiment; Among the 1000 simulations, what is the proportion of CIs including the true proportion p = 0.05? Calculate the average width of the 1000 CIs.*

Derivation of posterior distributions. Additionally includes the posterior mean as the theoretical point estimate.

![](assignments/hw-inference-part-c.png)

```{r}

# calculate point estimate (posterior mean) and 95% equal tails interval
# -> using uniform(0,1) = beta(1,1) prior
alpha <- 1
beta <- 1
data_results %<>% mutate(post_mean = (y + 1) / (n + alpha + beta),
                         bound_lower = qbeta((1 - c)/2, shape1 = y + alpha, shape2 = (n + beta) - y),
                         bound_upper = qbeta(1 - (1 - c)/2, shape1 = y + alpha, shape2 = (n + beta) - y),
                         capture = bound_lower < p & bound_upper > p)

# display first few CIs
data_results %>% 
  select(-ci_freq) %>% 
  display_nice(n = 5)

# nest results
data_results %<>% 
  nest(ci_bayes_uniform = c(post_mean, bound_lower, bound_upper, capture))
head(data_results, n = 5)

# calculate capture rate
data_results$ci_bayes_uniform %>% 
  reduce(bind_rows) %>% 
  summarize(avg_width = mean(bound_upper - bound_lower),
            capture_rate = mean(capture))

```

### Part d -- Comparison

*Compare your results in parts b and c, how similar are they? How different are they?*

Average width of the frequentist interval is smaller than that of the equal tails Bayes interval, which implies the capture rate should be higher for the latter. However it is quite a bit higher. 

### Part e -- New prior

*Repeat part d using a Beta(1, 10) prior of the parameter $p$.*

![](assignments/hw-inference-part-e.png)

```{r}

# calculate point estimate (posterior mean) and 95% HPD interval
# -> using beta(1,10)
alpha <- 1
beta <- 10
data_results %<>% mutate(post_mean = (y + 1) / (n + alpha + beta))
# TeachingDemos::hpd() not working in mutate()
data_results$bound_lower <- data_results$y %>% map(function(y) {
  TeachingDemos::hpd(qbeta, shape1 = y + alpha, shape2 = (n + beta) - y, conf = 0.95)[1]
})
data_results$bound_upper <- data_results$y %>% map(function(y) {
  TeachingDemos::hpd(qbeta, shape1 = y + alpha, shape2 = (n + beta) - y, conf = 0.95)[2]
})
data_results %<>% mutate(across(c(bound_lower, bound_upper), as.double), # simplify result type
                         capture = bound_lower < p & bound_upper > p)

# display first few CIs
data_results %>% 
  select(-c(ci_freq, ci_bayes_uniform)) %>% 
  display_nice(n = 5)

# nest results
data_results %<>% 
  nest(ci_bayes_beta = c(post_mean, bound_lower, bound_upper, capture))
head(data_results, n = 5)

# calculate capture rate
data_results$ci_bayes_beta %>% 
  reduce(bind_rows) %>% 
  summarize(avg_width = mean(bound_upper - bound_lower),
            capture_rate = mean(capture))

```

### Part f -- Final comparison

*Compare your results in parts b, c and e. How different are they? How similar are they? Why would you see these differences?*

```{r}

# display all summaries at same time
data_results %>% 
  select(starts_with("ci")) %>% 
  map(\(col) reduce(col, bind_rows)) %>% 
  map2(c("freq", "bayes_uniform", "bayes_beta"),
       \(df, nm) summarize(df, avg_width = mean(bound_upper - bound_lower) %>% round(3),
                            capture_rate = mean(capture)) %>% 
         mutate(method = nm,
                .before = 1)) %>% 
  reduce(bind_rows)

```

Bayes beta(10,1) resulted in the smallest intervals on average, which makes sense because the prior distribution incorporated good knowledge of the true population proportion. It also had a capture rate almost as high as the, on average, wider confidence intervals from the Bayes with uniform prior.

### Part g -- Additional insightful plots

Comparison of priors.

```{r}

# compare different prior distributions for the Bayesian credible intervals
ggplot() + 
  geom_line(aes(x = seq(from = 0, to = 1, by = 0.001),
                y = dunif(seq(from = 0, to = 1, by = 0.001), min = 0, max = 1)),
            color = "green") +
  geom_line(aes(x = seq(from = 0, to = 1, by = 0.001),
                y = dbeta(seq(from = 0, to = 1, by = 0.001), shape1 = 1, shape2 = 10)),
            color = "blue") + 
  annotate("text",
           x = Inf, y = Inf,
           hjust = 1, vjust = 1,
           label = "Uniform(0,1)",
           color = "green") + 
  annotate("text",
           x = Inf, y = Inf,
           hjust = 1, vjust = 3,
           label = "Beta(1,10)",
           color = "blue") + 
  labs(title = "Prior distributions",
       x = expression(theta),
       y = expression(pi(theta)))

```

Comparison of point estimates ($\hat{p}$ for frequentist and $E(\theta \mid y)$ for Bayes).

```{r}

# comparative boxplots of point estiamtes
data_results %>% 
  select(starts_with("ci")) %>% 
  map(\(col) reduce(col, bind_rows)) %>% 
  map2(c("freq", "bayes_uniform", "bayes_beta"),
       \(df, nm) select(df, pe = 1) %>% # select and rename point estimate
         mutate(method = nm,
                .before = 1)) %>% 
  reduce(bind_rows) %>% 
  ggplot(aes(x = pe, y = method)) + 
  geom_boxplot() + 
  geom_vline(xintercept = p,
             col = "blue") + 
  annotate("text",
           x = Inf, y = -Inf,
           hjust = 1, vjust = -1, 
           label = "true parameter",
           color = "blue") + 
  labs(x = "Point estimate")

```