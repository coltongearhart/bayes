# Lab - Single Parameter Inference

```{r}
#| label: load-prereqs
#| echo: false
#| message: false

# knitr options
source("_common.R")

```

### Setup

Let’s revisit the coin toss example today. We will work on it with multiple approaches. This time, let’s just assume we tossed the coin 100 times and get 50 ($Y = 50$) heads.

-   Data distribution: $Y \mid p \sim Bin(100, p)$
-   Prior: $p \sim \text{Uniform}(0,1)$
-   Observed data: $y = 50$

### Step 1 -- Estimation of parameters using known posterior distribution

If we know the math and used integration to find the posterior:

-   Posterior: $p \mid y = 50 \sim \text{Beta}(51, 51)$
-   Simulate the outcomes of the 100 tosses.

```{r}

# shape of the posterior distribution
curve(dbeta(x, shape1 = 51, shape2 = 51), from = 0, to = 1)

```

```{r}

# point estimates
(posterior_mean <- 51 / 102)
(posterior_median <- qbeta(p = 0.50, shape1 = 51, shape2 = 51))

x <- seq(from = 0, to = 1, length = 300)
d_posterior <- dbeta(x, shape1 = 51, shape2 = 51)
(posterior_mode <- x[which.max(d_posterior)])

```

```{r}

# credible intervals

# quantile based 95% confidence interval 
qbeta(p = c(0.025,0.975), shape1 = 51, shape2 = 51)

# 95% HPD CI
TeachingDemos::hpd(qbeta, shape1 = 51, shape2 = 51, conf = 0.95)

```

### Step 2 -- Estimation of parameters using simulation

Simulation is commonly used when the posterior doesn't have an explicit form. This process is illustrated below (although this one does have a closed form solution that we can use to validate the results).

-   Note that we can always write down the posterior distribution because it is just the product of the data distribution and the prior (which we always know / assume), but often the explicit for of the posterior is not a *known* form (e.g. $\sim \text{Beta}$ or $\sim \text{Normal}$.

If we do not know the math, we can still find the posterior, but we won’t have the exact posterior distribution, instead we are able to generate posterior samples. This is easy to implement and use when we have a simple model and single parameter.

-   Firstly, we create a sequence of possible values of the parameter (discretize the parameter space of $p$).

    -   Note that the method of discretizing the parameter space is not a very good method in practice and it cannot be used when the parameter space is unbounded.

-   Secondly, we construct the prior density values for each of the possible values and using Bayes Theorem, we combine the prior density and likelihood function into the posterior.

    -   Note that we would numerically sum the products of prior density and likelihood values to find the normalizing constant $p(y)$.

-   Thirdly, simulate random draws from the approximate posterior distribution.

![](assignments/lab-single-parameter-inference-simulation-process.png)

```{r}

# 1) create a sequence of possible values for the parameter
p <- seq(from = 0.001, to = 0.999, by = 0.001)

# 2) construct the posterior, assuming y = 50 is observed
# => posterior = data dist * prior dist
# -> getting the functional value at each p
f_p_post <- dbinom(x = 50, size = 100, prob = p) * dunif(p)

# 3) simulate p from the posterior distribution
# -> randomly sample according to weights
n_sim <- 100000
p_post <- sample(x = p, size = n_sim, replace = TRUE, prob = f_p_post)

# view approximate distribution
hist(p_post)

```

```{r}

# point estimates
(posterior_mean_sim <- mean(p_post))
(posterior_median_sim <- median(p_post))
(posterior_mode_sim <- p[which.max(f_p_post)])

```

```{r}

# credible interval
# -> quantile based 95% confidence interval 
quantile(p_post, c(0.025, 0.975))

```

### Step 3 -- Simulation-based prediction of future observations

You certainly can predict your future observations based on the posterior predictive distribution by doing the math here. But it is often difficult to analytically solve with calculus. On the opposite, it is fairly easy to get the posterior predictive distribution using simulation.

Note that we have simulated $p$ from the posterior distribution and saved them in the vector `p_post`, so let’s directly use them and simulate the future observation conditional on each one of these $p$’s to get an idea of the posterior predictive distribution.

- `p_post` can also be simulated directly from $\text{Beta}(51, 51)$ (the posterior distribution shown in @notes-inference) via `p_post = rbeta(nsim, 51,51)`. But the process described above is an approximation of simulating from this.

**One toss**: We already analytically figured out the posterior predictive distribution of 1 additional coin toss in class, so now we can numerically compare ̃the simulation vs analytical answer ($P(\tilde{Y} = 1 \mid Y = 50) = 0.5$):

```{r}

# generate an experimental value of the coin toss for each of the n_sim = 100,000 values of p from the posterior distribution
# -> need to make sure that the number of predicted responses should match the number of posterior samples of the parameter!
y_pred_post_1_sim <- rbinom(n = n_sim, size = 1, prob = p_post)

# summarize the posterior predictive distribution
# -> frequency table of the future observations table
table(y_pred_post_1_sim)

# calculate the naive posterior predictive distribution table using relative frequency
table(y_pred_post_1_sim) / length(y_pred_post_1_sim)

```

**5 tosses**: Now let’s assume that we plan to toss the coin for another 5 times.

```{r}

# same as above, except for 5 tosses
y_pred_post_5_sim <- rbinom(n = n_sim, size = 5, prob = p_post)
table(y_pred_post_5_sim)
table(y_pred_post_5_sim) / length(y_pred_post_5_sim)

```

![](assignments/lab-single-parameter-inference-derivation.png)

```{r}

# calculate analytical result
# -> y-pred_post | y-vec = 101! (50 + y-tilde)! (55 - y-tilde)! 5! / (50! 50! 106! y-tilde! (5 - y-tilde)!)
y_pred_post_5 <- function(y) {
  factorial(101) * factorial(50+y) * factorial(55-y) *  factorial(5) / (factorial(50) * factorial(50) * factorial(106) * factorial(y) * factorial(5-y))
}

# calculate probabilities for each possible y-tilde value (number of successes out of 5 tosses)
sapply(0:5, y_pred_post_5)

```

**100 tosses**: Let’s assume that we plan to toss the coin for another 100 times first.

```{r}

# same as above, except for 100 tosses
y_pred_post_100_sim <- rbinom(n = n_sim, size = 100, prob = p_post)

# create pmf plot
# -> sort predicted values to match order returned from table()
plot(x = y_pred_post_100_sim %>% unique %>% sort,
     y = y_pred_post_100_sim %>% table %>% as.numeric %>% divide_by(n_sim),
     type = "h", xlab = "Y-pred", ylab = "freq")

```

```{r}

# calculate analytical result
# -> (not simplifying to factorials) y-pred_post | y-vec = C(100,y-tilde) beta(51 + y-tilde, 51 - y-tilde) / beta(51, 51)

y_pred_post_100 <- function(y) {

  choose(100,y) * beta(51+y,151-y)/beta(51,51)

}

# calculate probabilities for each possible y-tilde value (number of successes out of 100 tosses)
sapply(0:100, y_pred_post_100)

# compare to simulated results
# -> get the theoretical functional values for the unique simulated posterior predictive values
plot(x = y_pred_post_100_sim %>% unique %>% sort %>% sapply(y_pred_post_100),
     y = y_pred_post_100_sim %>% table %>% as.numeric %>% divide_by(n_sim), 
     xlab = "analytical", ylab = "simulated")
abline(0,1)

```

```{r}

# point estimation/prediction for the future observations
mean(y_pred_post_100_sim)

# prediction interval
quantile(y_pred_post_100_sim, c(0.025,0.975))

```

Compare the posterior predictive samples vs the prior predictive samples. First we need to simulate $p$ from the prior distribution, then simulate the future observation conditional on each one of these $p$’s to get an idea of the prior predictive distribution.

```{r}

# generate an experimental value of the future 100 tosses for each p
p_prior <- sample(x = p, size = n_sim, replace = TRUE, prob = dunif(p))
y_pred_prior_100_sim <- rbinom(n = n_sim, size = 100, prob = p_prior)

# calculate point estimate and interval
mean(y_pred_prior_100_sim)
quantile(y_pred_prior_100_sim, c(.025,.975))

```

```{r}

# compare with what we got earlier:

# compare the two distribution via density curves 
plot(density(y_pred_post_100_sim), xlim = c(0,100), main = "Prior/posterior pred. dist. of 100 future tosses")
lines(density(y_pred_prior_100_sim),col = "red")

```
